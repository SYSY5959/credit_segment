{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 858)\n",
      "(500000, 858)\n",
      "(500000, 858)\n",
      "(500000, 858)\n",
      "(500000, 858)\n",
      "(500000, 858)\n"
     ]
    }
   ],
   "source": [
    "# Read the parquet file\n",
    "dfs={}\n",
    "\n",
    "dfs[7] = pd.read_parquet('../data/monthly_merge_data/total/201807_total.parquet')\n",
    "dfs[8] = pd.read_parquet('../data/monthly_merge_data/total/201808_total.parquet')\n",
    "dfs[9] = pd.read_parquet('../data/monthly_merge_data/total/201809_total.parquet')\n",
    "dfs[10] = pd.read_parquet('../data/monthly_merge_data/total/201810_total.parquet')\n",
    "dfs[11] = pd.read_parquet('../data/monthly_merge_data/total/201811_total.parquet')\n",
    "dfs[12] = pd.read_parquet('../data/monthly_merge_data/total/201812_total.parquet')\n",
    "\n",
    "for i in range(7,13):\n",
    "    print(dfs[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.90 GiB for an array with shape (85, 3000000) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Concatenate the dataframes\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\howsr\\miniconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\howsr\\miniconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\howsr\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:177\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    167\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ea_compat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.90 GiB for an array with shape (85, 3000000) and data type int64"
     ]
    }
   ],
   "source": [
    "# Concatenate the dataframes\n",
    "df = pd.concat([dfs[7], dfs[8], dfs[9], dfs[10], dfs[11], dfs[12]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000000, 858)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "#dfs를 메모리에서 삭제\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Column Classification by Data Type ===\n",
      "Number of numeric columns: 808\n",
      "Number of datetime columns: 0\n",
      "Number of categorical columns: 0\n",
      "Number of object columns: 50\n",
      "\n",
      "Sample numeric columns: ['기준년월', '남녀구분코드', '회원여부_이용가능', '회원여부_이용가능_CA', '회원여부_이용가능_카드론']\n",
      "\n",
      "Sample object columns: ['ID', '연령', 'Segment', '가입통신회사코드', '거주시도명']\n"
     ]
    }
   ],
   "source": [
    "# Classify columns by data type\n",
    "dtypes = df.dtypes\n",
    "numeric_cols = dtypes[dtypes.apply(lambda x: np.issubdtype(x, np.number))].index.tolist()\n",
    "datetime_cols = dtypes[dtypes == 'datetime64[ns]'].index.tolist()\n",
    "categorical_cols = dtypes[dtypes == 'category'].index.tolist()\n",
    "object_cols = dtypes[dtypes == 'object'].index.tolist()\n",
    "\n",
    "# Print the classification\n",
    "print(\"\\n=== Column Classification by Data Type ===\")\n",
    "print(f\"Number of numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"Number of datetime columns: {len(datetime_cols)}\")\n",
    "print(f\"Number of categorical columns: {len(categorical_cols)}\")\n",
    "print(f\"Number of object columns: {len(object_cols)}\")\n",
    "\n",
    "# Display sample columns of each type (first 5)\n",
    "if numeric_cols:\n",
    "    print(\"\\nSample numeric columns:\", numeric_cols[:5])\n",
    "if datetime_cols:\n",
    "    print(\"\\nSample datetime columns:\", datetime_cols[:5])\n",
    "if categorical_cols:\n",
    "    print(\"\\nSample categorical columns:\", categorical_cols[:5])\n",
    "if object_cols:\n",
    "    print(\"\\nSample object columns:\", object_cols[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " '연령',\n",
       " 'Segment',\n",
       " '가입통신회사코드',\n",
       " '거주시도명',\n",
       " '직장시도명',\n",
       " '_1순위신용체크구분',\n",
       " '_2순위신용체크구분',\n",
       " '연회비발생카드수_B0M',\n",
       " '상품관련면제카드수_B0M',\n",
       " '임직원면제카드수_B0M',\n",
       " '우수회원면제카드수_B0M',\n",
       " '기타면제카드수_B0M',\n",
       " 'Life_Stage',\n",
       " '자발한도감액횟수_R12M',\n",
       " '한도증액횟수_R12M',\n",
       " '카드론동의여부',\n",
       " 'RV전환가능여부',\n",
       " '한도심사요청건수',\n",
       " '_1순위업종',\n",
       " '_2순위업종',\n",
       " '_3순위업종',\n",
       " '_1순위쇼핑업종',\n",
       " '_2순위쇼핑업종',\n",
       " '_3순위쇼핑업종',\n",
       " '_1순위교통업종',\n",
       " '_2순위교통업종',\n",
       " '_3순위교통업종',\n",
       " '_1순위여유업종',\n",
       " '_2순위여유업종',\n",
       " '_3순위여유업종',\n",
       " '_1순위납부업종',\n",
       " '_2순위납부업종',\n",
       " '_3순위납부업종',\n",
       " '최종카드론_신청경로코드',\n",
       " '이용금액대',\n",
       " '대표결제방법코드',\n",
       " '대표청구지고객주소구분코드',\n",
       " '대표청구서수령지구분코드',\n",
       " '청구서수령방법',\n",
       " '할인건수_R3M',\n",
       " '할인건수_B0M',\n",
       " '인입횟수_ARS_R6M',\n",
       " '이용메뉴건수_ARS_R6M',\n",
       " '방문횟수_PC_R6M',\n",
       " '방문일수_PC_R6M',\n",
       " '방문횟수_앱_R6M',\n",
       " 'OS구분코드',\n",
       " '캠페인접촉건수_R12M',\n",
       " '캠페인접촉일수_R12M']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/monthly_merge_data/total/201807_total.parquet')\n",
    "df_small = df.iloc[:3000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검사한 전체 열 개수: 858\n",
      "작은 데이터셋에서 누락된 고유값이 있는 열 개수: 565\n",
      "\n",
      "=== 누락된 고유값이 있는 열 상세 정보 ===\n",
      "\n",
      "열 이름: Segment\n",
      "전체 데이터셋 고유값 개수: 6\n",
      "작은 데이터셋 고유값 개수: 4\n",
      "작은 데이터셋에서 누락된 고유값 개수: 2\n",
      "누락된 값 예시: [None, 'B']\n",
      "\n",
      "열 이름: 소지카드수_유효_신용\n",
      "전체 데이터셋 고유값 개수: 5\n",
      "작은 데이터셋 고유값 개수: 4\n",
      "작은 데이터셋에서 누락된 고유값 개수: 1\n",
      "누락된 값 예시: [4]\n",
      "\n",
      "열 이름: 입회일자_신용\n",
      "전체 데이터셋 고유값 개수: 326\n",
      "작은 데이터셋 고유값 개수: 289\n",
      "작은 데이터셋에서 누락된 고유값 개수: 37\n",
      "누락된 값 예시: [19920901, 19940101, 19930501, 20041101, 19910801]\n",
      "\n",
      "열 이름: 입회경과개월수_신용\n",
      "전체 데이터셋 고유값 개수: 335\n",
      "작은 데이터셋 고유값 개수: 289\n",
      "작은 데이터셋에서 누락된 고유값 개수: 46\n",
      "누락된 값 예시: [266, 281, 283, 284, 285]\n",
      "\n",
      "열 이름: 최종탈회후경과월\n",
      "전체 데이터셋 고유값 개수: 71\n",
      "작은 데이터셋 고유값 개수: 34\n",
      "작은 데이터셋에서 누락된 고유값 개수: 37\n",
      "누락된 값 예시: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# 각 데이터프레임의 고유값을 저장할 딕셔너리 생성\n",
    "unique_values_small = {}\n",
    "unique_values_full = {}\n",
    "\n",
    "# 두 데이터프레임의 각 열별 고유값 개수 계산\n",
    "for col in df.columns[2:]:\n",
    "    unique_values_small[col] = set(df_small[col].unique())\n",
    "    unique_values_full[col] = set(df[col].unique())\n",
    "\n",
    "# 고유값 비교\n",
    "missing_values = {}\n",
    "for col in df.columns[2:]:\n",
    "    missing = unique_values_full[col] - unique_values_small[col]\n",
    "    if len(missing) > 0:\n",
    "        missing_values[col] = missing\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"검사한 전체 열 개수: {len(df.columns)}\")\n",
    "print(f\"작은 데이터셋에서 누락된 고유값이 있는 열 개수: {len(missing_values)}\")\n",
    "\n",
    "# 누락된 값이 있는 열에 대한 상세 정보 출력\n",
    "if missing_values:\n",
    "    print(\"\\n=== 누락된 고유값이 있는 열 상세 정보 ===\")\n",
    "    counter = 0\n",
    "    for col, missing in missing_values.items():\n",
    "        if counter >= 5:  # 가독성을 위해 처음 5개 열만 표시\n",
    "            break\n",
    "        print(f\"\\n열 이름: {col}\")\n",
    "        print(f\"전체 데이터셋 고유값 개수: {len(unique_values_full[col])}\")\n",
    "        print(f\"작은 데이터셋 고유값 개수: {len(unique_values_small[col])}\")\n",
    "        print(f\"작은 데이터셋에서 누락된 고유값 개수: {len(missing)}\")\n",
    "        \n",
    "        # 누락된 값의 예시 표시 (최대 5개)\n",
    "        if len(missing) > 0:\n",
    "            print(\"누락된 값 예시:\", list(missing)[:5])\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 샘플링 된 데이터로 전처리 시작\n",
    "segment에 대해서만 다시 고려하면 될 듯 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df= df_small\n",
    "# df_copy = df.copy()\n",
    "\n",
    "#df_copy에 분리된 복제본 생성\n",
    "df = df_copy.copy()\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_values(df, column_name):\n",
    "    # Get unique values and their counts\n",
    "    value_counts = df[column_name].value_counts()\n",
    "    \n",
    "    # Check for NULL values\n",
    "    null_count = df[column_name].isnull().sum()\n",
    "    \n",
    "    print(f\"=== Analysis for column: {column_name} ===\")\n",
    "    print(\"Unique values and their counts:\")\n",
    "    for value, count in value_counts.items():\n",
    "        print(f\"{value}: {count}\")\n",
    "    \n",
    "    print(f\"Total unique values: {len(value_counts)}\")\n",
    "    print(f\"NULL values: {null_count}\")\n",
    "    if null_count > 0:\n",
    "        print(f\"NULL percentage: {(null_count/len(df))*100:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID 노터치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연령\n",
    "'20대', '30대', ... -> 20, 30 (str -> int)\n",
    "'70이상' -> 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 연령 ===\n",
      "Unique values and their counts:\n",
      "40대: 960\n",
      "30대: 751\n",
      "50대: 651\n",
      "60대: 300\n",
      "20대: 244\n",
      "70대이상: 94\n",
      "Total unique values: 6\n",
      "NULL values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '연령')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40 30 20 60 50 70]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# 나이 문자열을 정수로 변환하는 함수\n",
    "def convert_age(age_str):\n",
    "    if pd.isna(age_str):\n",
    "        return None\n",
    "        \n",
    "    # '20대', '30대' 등의 형식 처리\n",
    "    if '대' in age_str:\n",
    "        # '70대이상'과 같은 케이스도 처리\n",
    "        if '이상' in age_str:\n",
    "            return int(re.findall(r'\\d+', age_str)[0])\n",
    "        return int(age_str.replace('대', ''))\n",
    "    # '70이상' 형식 처리\n",
    "    elif '이상' in age_str:\n",
    "        return int(re.findall(r'\\d+', age_str)[0])\n",
    "    # 기타 형식인 경우 숫자만 추출\n",
    "    else:\n",
    "        digits = re.findall(r'\\d+', age_str)\n",
    "        if digits:\n",
    "            return int(digits[0])\n",
    "        return age_str\n",
    "\n",
    "# 모든 데이터프레임에 변환 적용\n",
    "df['연령'] = df['연령'].apply(convert_age)\n",
    "print(df['연령'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가입통신회사코드\n",
    "s,k,l에 대한 원핫 인코딩 수행\n",
    "결측치는 000이 들어가게 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 가입통신회사코드 ===\n",
      "Unique values and their counts:\n",
      "S사: 1245\n",
      "K사: 678\n",
      "L사: 556\n",
      "Total unique values: 3\n",
      "NULL values: 521\n",
      "NULL percentage: 17.37%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '가입통신회사코드')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 통신사 더미 변수 열: ['통신사_K사', '통신사_L사', '통신사_S사']\n"
     ]
    }
   ],
   "source": [
    "# '가입통신회사코드'에 대한 원핫인코딩 수행\n",
    "# 접두어 '통신사_'를 사용하여 새로운 열 이름 생성\n",
    "telecom_dummies = pd.get_dummies(df['가입통신회사코드'], prefix='통신사')\n",
    "\n",
    "# 원본 열을 삭제하고 더미 변수를 데이터프레임에 연결\n",
    "df = pd.concat([df.drop('가입통신회사코드', axis=1), telecom_dummies], axis=1)\n",
    "\n",
    "# 통신사 더미 변수 열에서 결측치를 0으로 채움\n",
    "telecom_cols = [col for col in df.columns if col.startswith('통신사_')]\n",
    "df[telecom_cols] = df[telecom_cols].fillna(0)\n",
    "\n",
    "# 결과 확인을 위해 새로 생성된 통신사 열 출력\n",
    "print(\"생성된 통신사 더미 변수 열:\", telecom_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 통신사_K사 ===\n",
      "Unique values and their counts:\n",
      "False: 2322\n",
      "True: 678\n",
      "Total unique values: 2\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 통신사_L사 ===\n",
      "Unique values and their counts:\n",
      "False: 2444\n",
      "True: 556\n",
      "Total unique values: 2\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 통신사_S사 ===\n",
      "Unique values and their counts:\n",
      "False: 1755\n",
      "True: 1245\n",
      "Total unique values: 2\n",
      "NULL values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '통신사_K사')\n",
    "check_unique_values(df, '통신사_L사')\n",
    "check_unique_values(df, '통신사_S사')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 거주시도도명, 직장시도명\n",
    "원핫인코딩 수행\n",
    "(!) 일치여부에 따른 파생변수가 있어도 괜찮지 않을까 생각해서 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 거주시도명 ===\n",
      "Unique values and their counts:\n",
      "서울: 879\n",
      "경기: 796\n",
      "대전: 199\n",
      "인천: 199\n",
      "부산: 194\n",
      "충북: 124\n",
      "경북: 117\n",
      "경남: 106\n",
      "대구: 98\n",
      "울산: 82\n",
      "전남: 66\n",
      "광주: 61\n",
      "전북: 32\n",
      "충남: 27\n",
      "강원: 19\n",
      "제주: 1\n",
      "Total unique values: 16\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 직장시도명 ===\n",
      "Unique values and their counts:\n",
      "경기: 733\n",
      "서울: 730\n",
      "인천: 176\n",
      "대전: 169\n",
      "부산: 165\n",
      "충북: 118\n",
      "경북: 109\n",
      "경남: 107\n",
      "대구: 94\n",
      "울산: 73\n",
      "광주: 60\n",
      "전남: 59\n",
      "전북: 48\n",
      "충남: 26\n",
      "강원: 25\n",
      "세종: 9\n",
      "제주: 3\n",
      "Total unique values: 17\n",
      "NULL values: 296\n",
      "NULL percentage: 9.87%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '거주시도명')\n",
    "check_unique_values(df, '직장시도명')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 거주시도 더미 변수 열: ['거주_강원', '거주_경기', '거주_경남', '거주_경북', '거주_광주', '거주_대구', '거주_대전', '거주_부산', '거주_서울', '거주_울산', '거주_인천', '거주_전남', '거주_전북', '거주_제주', '거주_충남', '거주_충북']\n",
      "생성된 직장시도 더미 변수 열: ['직장_강원', '직장_경기', '직장_경남', '직장_경북', '직장_광주', '직장_대구', '직장_대전', '직장_부산', '직장_서울', '직장_세종', '직장_울산', '직장_인천', '직장_전남', '직장_전북', '직장_제주', '직장_충남', '직장_충북']\n"
     ]
    }
   ],
   "source": [
    "# '거주시도명'에 대한 원핫인코딩 수행\n",
    "residence_dummies = pd.get_dummies(df['거주시도명'], prefix='거주')\n",
    "residence_dummies = residence_dummies.astype(int)  # boolean을 0,1로 변환\n",
    "\n",
    "# '직장시도명'에 대한 원핫인코딩 수행\n",
    "workplace_dummies = pd.get_dummies(df['직장시도명'], prefix='직장')\n",
    "workplace_dummies = workplace_dummies.astype(int)  # boolean을 0,1로 변환\n",
    "\n",
    "# 원본 열을 삭제하고 더미 변수를 데이터프레임에 연결\n",
    "df = pd.concat([df.drop(['거주시도명', '직장시도명'], axis=1), \n",
    "                residence_dummies, \n",
    "                workplace_dummies], axis=1)\n",
    "\n",
    "# 결과 확인을 위해 새로 생성된 열 출력\n",
    "residence_cols = [col for col in df.columns if col.startswith('거주_')]\n",
    "workplace_cols = [col for col in df.columns if col.startswith('직장_')]\n",
    "print(\"생성된 거주시도 더미 변수 열:\", residence_cols)\n",
    "print(\"생성된 직장시도 더미 변수 열:\", workplace_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "거주지와 직장 일치 여부:\n",
      "거주직장일치\n",
      "1    2589\n",
      "0     411\n",
      "Name: count, dtype: int64\n",
      "일치 비율: 86.30%\n"
     ]
    }
   ],
   "source": [
    "# 거주지와 직장 일치 여부를 나타내는 변수 생성\n",
    "# 지역명에서 접두어 추출\n",
    "residence_prefixes = [col.replace('거주_', '') for col in residence_cols]\n",
    "workplace_prefixes = [col.replace('직장_', '') for col in workplace_cols]\n",
    "\n",
    "# 공통 지역 식별\n",
    "common_regions = list(set(residence_prefixes) & set(workplace_prefixes))\n",
    "\n",
    "# 거주지-직장 일치 여부 변수 초기화\n",
    "df['거주직장일치'] = 0\n",
    "\n",
    "# 각 지역에 대해 거주지와 직장이 일치하는지 확인\n",
    "for region in common_regions:\n",
    "    residence_col = f'거주_{region}'\n",
    "    workplace_col = f'직장_{region}'\n",
    "    \n",
    "    # 해당 지역의 거주지와 직장이 모두 1인 경우 일치로 표시\n",
    "    matches = (df[residence_col] == 1) & (df[workplace_col] == 1)\n",
    "    df.loc[matches, '거주직장일치'] = 1\n",
    "\n",
    "# 결과 확인\n",
    "print(\"거주지와 직장 일치 여부:\")\n",
    "print(df['거주직장일치'].value_counts())\n",
    "print(f\"일치 비율: {df['거주직장일치'].mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  '_1순위신용체크구분', '_2순위신용체크구분'\n",
    "원핫인코딩 수행행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: _1순위신용체크구분 ===\n",
      "Unique values and their counts:\n",
      "신용: 2894\n",
      "체크: 90\n",
      "Total unique values: 2\n",
      "NULL values: 16\n",
      "NULL percentage: 0.53%\n",
      "\n",
      "=== Analysis for column: _2순위신용체크구분 ===\n",
      "Unique values and their counts:\n",
      "신용: 1089\n",
      "체크: 705\n",
      "Total unique values: 2\n",
      "NULL values: 1206\n",
      "NULL percentage: 40.20%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '_1순위신용체크구분')\n",
    "check_unique_values(df, '_2순위신용체크구분')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두 열이 동일한 값이 있는 행 개수: 1032\n",
      "두 열이 동일한 값이 있는 행 비율: 34.40%\n"
     ]
    }
   ],
   "source": [
    "# 두 열이 동일한 값이 신용으로 있는 행이 있는지 확인\n",
    "credit_check_match = (df['_1순위신용체크구분'] == df['_2순위신용체크구분'])\n",
    "credit_check_match_count = credit_check_match.sum()\n",
    "credit_check_match_rate = credit_check_match.mean()\n",
    "\n",
    "print(f\"두 열이 동일한 값이 있는 행 개수: {credit_check_match_count}\")\n",
    "print(f\"두 열이 동일한 값이 있는 행 비율: {credit_check_match_rate * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Unique combinations and their counts ===\n",
      "  _1순위신용체크구분 _2순위신용체크구분  count\n",
      "0         신용         신용   1017\n",
      "1         신용         체크    690\n",
      "2         체크         신용     72\n",
      "3         체크         체크     15\n",
      "\n",
      "=== Null value counts ===\n",
      "_1순위신용체크구분: 16\n",
      "_2순위신용체크구분: 1206\n",
      "\n",
      "=== Combinations with percentages ===\n",
      "  _1순위신용체크구분 _2순위신용체크구분  count  percentage\n",
      "0         신용         신용   1017        33.9\n",
      "1         신용         체크    690        23.0\n",
      "2         체크         신용     72         2.4\n",
      "3         체크         체크     15         0.5\n"
     ]
    }
   ],
   "source": [
    "# Get unique combinations of the two columns\n",
    "combinations = df[['_1순위신용체크구분', '_2순위신용체크구분']].drop_duplicates()\n",
    "\n",
    "# Count occurrences of each combination\n",
    "combination_counts = df.groupby(['_1순위신용체크구분', '_2순위신용체크구분']).size().reset_index(name='count')\n",
    "\n",
    "print(\"=== Unique combinations and their counts ===\")\n",
    "print(combination_counts.sort_values('count', ascending=False))\n",
    "\n",
    "# Check if there are any null values\n",
    "print(\"\\n=== Null value counts ===\")\n",
    "print(\"_1순위신용체크구분:\", df['_1순위신용체크구분'].isnull().sum())\n",
    "print(\"_2순위신용체크구분:\", df['_2순위신용체크구분'].isnull().sum())\n",
    "\n",
    "# Calculate percentage of each combination\n",
    "total = len(df)\n",
    "combination_counts['percentage'] = (combination_counts['count'] / total * 100).round(2)\n",
    "print(\"\\n=== Combinations with percentages ===\")\n",
    "print(combination_counts.sort_values('percentage', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신용체크1 더미 변수: ['신용체크1_신용', '신용체크1_체크']\n",
      "신용체크2 더미 변수: ['신용체크2_신용', '신용체크2_체크']\n"
     ]
    }
   ],
   "source": [
    "# Create boolean one-hot encoding for both columns\n",
    "credit_check1_dummies = pd.get_dummies(df['_1순위신용체크구분'], prefix='신용체크1')\n",
    "credit_check2_dummies = pd.get_dummies(df['_2순위신용체크구분'], prefix='신용체크2')\n",
    "\n",
    "# Convert to boolean (True/False)\n",
    "credit_check1_dummies = credit_check1_dummies.astype(bool)\n",
    "credit_check2_dummies = credit_check2_dummies.astype(bool)\n",
    "\n",
    "# Drop original columns and concatenate the dummy variables\n",
    "df = pd.concat([df.drop(['_1순위신용체크구분', '_2순위신용체크구분'], axis=1), \n",
    "                credit_check1_dummies, \n",
    "                credit_check2_dummies], axis=1)\n",
    "\n",
    "# Print the new column names\n",
    "credit_check1_cols = [col for col in df.columns if col.startswith('신용체크1_')]\n",
    "credit_check2_cols = [col for col in df.columns if col.startswith('신용체크2_')]\n",
    "print(\"신용체크1 더미 변수:\", credit_check1_cols)\n",
    "print(\"신용체크2 더미 변수:\", credit_check2_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  '연회비발생카드수_B0M', '상품관련면제카드수_B0M', '임직원면제카드수_B0M', '우수회원면제카드수_B0M', '기타면제카드수_B0M',\n",
    "연회비발생카드수_B0M는 원핫 인코딩 나머지는 제거\n",
    "사유: 모두 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 연회비발생카드수_B0M ===\n",
      "Unique values and their counts:\n",
      "0개: 2965\n",
      "1개이상: 35\n",
      "Total unique values: 2\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 상품관련면제카드수_B0M ===\n",
      "Unique values and their counts:\n",
      "0개: 3000\n",
      "Total unique values: 1\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 임직원면제카드수_B0M ===\n",
      "Unique values and their counts:\n",
      "0개: 3000\n",
      "Total unique values: 1\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 우수회원면제카드수_B0M ===\n",
      "Unique values and their counts:\n",
      "0개: 3000\n",
      "Total unique values: 1\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 기타면제카드수_B0M ===\n",
      "Unique values and their counts:\n",
      "0개: 3000\n",
      "Total unique values: 1\n",
      "NULL values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '연회비발생카드수_B0M')\n",
    "check_unique_values(df, '상품관련면제카드수_B0M')\n",
    "check_unique_values(df, '임직원면제카드수_B0M')\n",
    "check_unique_values(df, '우수회원면제카드수_B0M')\n",
    "check_unique_values(df, '기타면제카드수_B0M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[('상품관련면제카드수_B0M', '임직원면제카드수_B0M', '우수회원면제카드수_B0M', '기타면제카드수_B0M')] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m연회비발생카드수_B0M\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m연회비발생카드수_B0M\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0개\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m연회비발생카드수_B0M\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m상품관련면제카드수_B0M\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m임직원면제카드수_B0M\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m우수회원면제카드수_B0M\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m기타면제카드수_B0M\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\howsr\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\howsr\\miniconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\howsr\\miniconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\howsr\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"[('상품관련면제카드수_B0M', '임직원면제카드수_B0M', '우수회원면제카드수_B0M', '기타면제카드수_B0M')] not found in axis\""
     ]
    }
   ],
   "source": [
    "#연회비발생카드수_B0M이 '0개'이면 0으로 변환 나머진 1로 변환\n",
    "df['연회비발생카드수_B0M'] = df['연회비발생카드수_B0M'].apply(lambda x: 0 if x == '0개' else 1)\n",
    "df['연회비발생카드수_B0M'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(['상품관련면제카드수_B0M', '임직원면제카드수_B0M', '우수회원면제카드수_B0M', '기타면제카드수_B0M'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 연회비발생카드수_B0M ===\n",
      "Unique values and their counts:\n",
      "0: 2965\n",
      "1: 35\n",
      "Total unique values: 2\n",
      "NULL values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '연회비발생카드수_B0M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 'Life_Stage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: Life_Stage ===\n",
      "Unique values and their counts:\n",
      "자녀성장(1): 1029\n",
      "자녀성장(2): 735\n",
      "자녀출산기: 329\n",
      "가족구축기: 283\n",
      "노년생활: 270\n",
      "자녀독립기: 235\n",
      "독신: 119\n",
      "Total unique values: 7\n",
      "NULL values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, 'Life_Stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: Life_Stage ===\n",
      "Unique values and their counts:\n",
      "자녀성장(1): 1029\n",
      "자녀성장(2): 735\n",
      "자녀출산기: 329\n",
      "가족구축기: 283\n",
      "노년생활: 270\n",
      "자녀독립기: 235\n",
      "독신: 119\n",
      "Total unique values: 7\n",
      "NULL values: 0\n",
      "\n",
      "생성된 Life Stage 더미 변수 열: ['LifeStage_가족구축기', 'LifeStage_노년생활', 'LifeStage_독신', 'LifeStage_자녀독립기', 'LifeStage_자녀성장(1)', 'LifeStage_자녀성장(2)', 'LifeStage_자녀출산기']\n",
      "총 7개의 Life Stage 더미 변수가 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# Life_Stage에 대한 원핫 인코딩 수행\n",
    "# 먼저 고유 값 확인\n",
    "check_unique_values(df, 'Life_Stage')\n",
    "\n",
    "# 원핫 인코딩 수행\n",
    "life_stage_dummies = pd.get_dummies(df['Life_Stage'], prefix='LifeStage')\n",
    "life_stage_dummies = life_stage_dummies.astype(int)  # boolean을 0,1로 변환\n",
    "\n",
    "# 원본 열을 삭제하고 더미 변수를 데이터프레임에 연결\n",
    "df = pd.concat([df.drop('Life_Stage', axis=1), life_stage_dummies], axis=1)\n",
    "\n",
    "# 결과 확인을 위해 새로 생성된 열 출력\n",
    "life_stage_cols = [col for col in df.columns if col.startswith('LifeStage_')]\n",
    "print(\"생성된 Life Stage 더미 변수 열:\", life_stage_cols)\n",
    "print(f\"총 {len(life_stage_cols)}개의 Life Stage 더미 변수가 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## '자발한도감액횟수_R12M', '한도증액횟수_R12M'\n",
    "0회 -> 0\n",
    "1회 -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 자발한도감액횟수_R12M ===\n",
      "Unique values and their counts:\n",
      "0회: 2991\n",
      "1회: 9\n",
      "Total unique values: 2\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 한도증액횟수_R12M ===\n",
      "Unique values and their counts:\n",
      "0회: 2667\n",
      "1회이상: 333\n",
      "Total unique values: 2\n",
      "NULL values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '자발한도감액횟수_R12M')\n",
    "check_unique_values(df, '한도증액횟수_R12M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자발한도감액횟수_R12M\n",
      "0    2991\n",
      "1       9\n",
      "Name: count, dtype: int64\n",
      "한도증액횟수_R12M\n",
      "0    2667\n",
      "1     333\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['자발한도감액횟수_R12M'] = df['자발한도감액횟수_R12M'].apply(lambda x: 0 if x == '0회' else 1)\n",
    "df['한도증액횟수_R12M'] = df['한도증액횟수_R12M'].apply(lambda x: 0 if x == '0회' else 1)\n",
    "print(df['자발한도감액횟수_R12M'].value_counts())\n",
    "print(df['한도증액횟수_R12M'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## '카드론동의여부', 'RV전환가능여부', '한도심사요청건수\n",
    "Y -> 0, N -> 1으로 변환\n",
    "Z -> 0, N -> 1으로 변환\n",
    "회 제거거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 카드론동의여부 ===\n",
      "Unique values and their counts:\n",
      "Y: 2414\n",
      "N: 586\n",
      "Total unique values: 2\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: RV전환가능여부 ===\n",
      "Unique values and their counts:\n",
      "Z: 2484\n",
      "N: 516\n",
      "Total unique values: 2\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 한도심사요청건수 ===\n",
      "Unique values and their counts:\n",
      "0회: 3000\n",
      "Total unique values: 1\n",
      "NULL values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '카드론동의여부')\n",
    "check_unique_values(df, 'RV전환가능여부')\n",
    "check_unique_values(df, '한도심사요청건수')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카드론동의여부\n",
      "0    2414\n",
      "1     586\n",
      "Name: count, dtype: int64\n",
      "RV전환가능여부\n",
      "0    2484\n",
      "1     516\n",
      "Name: count, dtype: int64\n",
      "한도심사요청건수\n",
      "0    3000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['카드론동의여부'] = df['카드론동의여부'].apply(lambda x: 0 if x == 'Y' else 1)\n",
    "df['RV전환가능여부'] = df['RV전환가능여부'].apply(lambda x: 0 if x == 'Z' else 1)\n",
    "df['한도심사요청건수'] = df['한도심사요청건수'].str.replace('회', '').astype(int)\n",
    "\n",
    "print(df['카드론동의여부'].value_counts())\n",
    "print(df['RV전환가능여부'].value_counts())\n",
    "print(df['한도심사요청건수'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## '_1순위업종', '_2순위업종', '_3순위업종, ...\n",
    "업종 명으로 열을 만들고 열에 이용금액 열의 값을 활용하여 채워 넣는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: _1순위업종 ===\n",
      "Unique values and their counts:\n",
      "쇼핑: 1351\n",
      "납부: 511\n",
      "교통: 224\n",
      "사교활동: 165\n",
      "교육: 99\n",
      "의료: 44\n",
      "해외: 38\n",
      "여유생활: 7\n",
      "일상생활: 2\n",
      "Total unique values: 9\n",
      "NULL values: 559\n",
      "NULL percentage: 18.63%\n",
      "\n",
      "=== Analysis for column: _2순위업종 ===\n",
      "Unique values and their counts:\n",
      "사교활동: 683\n",
      "쇼핑: 416\n",
      "납부: 354\n",
      "교통: 287\n",
      "의료: 103\n",
      "교육: 65\n",
      "해외: 43\n",
      "여유생활: 18\n",
      "일상생활: 8\n",
      "Total unique values: 9\n",
      "NULL values: 1023\n",
      "NULL percentage: 34.10%\n",
      "\n",
      "=== Analysis for column: _3순위업종 ===\n",
      "Unique values and their counts:\n",
      "사교활동: 520\n",
      "교통: 503\n",
      "납부: 221\n",
      "쇼핑: 166\n",
      "의료: 152\n",
      "여유생활: 55\n",
      "해외: 41\n",
      "일상생활: 27\n",
      "교육: 17\n",
      "요식: 16\n",
      "Total unique values: 10\n",
      "NULL values: 1282\n",
      "NULL percentage: 42.73%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '_1순위업종')\n",
    "check_unique_values(df, '_2순위업종')\n",
    "check_unique_values(df, '_3순위업종')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Unique industries ===\n",
      "전체 업종: ['납부', '쇼핑', None, '사교활동', '교육', '교통', '해외', '의료', '여유생활', '일상생활', '요식']\n",
      "쇼핑 업종: ['쇼핑기타', '도소매', '온라인', '마트', None, '슈퍼마켓', '편의점', '백화점', '아울렛']\n",
      "교통 업종: ['택시', '주유', None, '버스지하철', '정비', '철도버스']\n",
      "여유 업종: [None, '공연', '여유기타', '운동', 'Pet', '항공', '숙박', '공원']\n",
      "납부 업종: ['보험료', '통신비', None, '관리비', '납부기타', '가스/전기료']\n"
     ]
    }
   ],
   "source": [
    "# 모든 업종의 고유값 추출\n",
    "all_industries = pd.concat([\n",
    "    df['_1순위업종'],\n",
    "    df['_2순위업종'],\n",
    "    df['_3순위업종']\n",
    "]).unique().tolist()\n",
    "shoping = pd.concat([\n",
    "    df['_1순위쇼핑업종'],\n",
    "    df['_2순위쇼핑업종'],\n",
    "    df['_3순위쇼핑업종']\n",
    "]).unique().tolist()\n",
    "traffic = pd.concat([\n",
    "    df['_1순위교통업종'],\n",
    "    df['_2순위교통업종'],\n",
    "    df['_3순위교통업종']\n",
    "]).unique().tolist()\n",
    "free = pd.concat([\n",
    "    df['_1순위여유업종'],\n",
    "    df['_2순위여유업종'],\n",
    "    df['_3순위여유업종']\n",
    "]).unique().tolist()\n",
    "pay = pd.concat([\n",
    "    df['_1순위납부업종'],\n",
    "    df['_2순위납부업종'],\n",
    "    df['_3순위납부업종']\n",
    "]).unique().tolist()\n",
    "\n",
    "print(\"=== Unique industries ===\")\n",
    "print(\"전체 업종:\", all_industries)\n",
    "print(\"쇼핑 업종:\", shoping)\n",
    "print(\"교통 업종:\", traffic)\n",
    "print(\"여유 업종:\", free)\n",
    "print(\"납부 업종:\", pay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Unique industries ===\n",
      "전체 업종: ['납부', '쇼핑', '사교활동', '교육', '교통', '해외', '의료', '여유생활', '일상생활', '요식']\n",
      "쇼핑 업종: ['쇼핑기타', '도소매', '온라인', '마트', '슈퍼마켓', '편의점', '백화점', '아울렛']\n",
      "교통 업종: ['택시', '주유', '버스지하철', '정비', '철도버스']\n",
      "여유 업종: ['공연', '여유기타', '운동', 'Pet', '항공', '숙박', '공원']\n",
      "납부 업종: ['보험료', '통신비', '관리비', '납부기타', '가스/전기료']\n"
     ]
    }
   ],
   "source": [
    "#각 리스트에 None 값을 제외\n",
    "all_industries = [industry for industry in all_industries if industry is not None]\n",
    "shoping = [industry for industry in shoping if industry is not None]\n",
    "traffic = [industry for industry in traffic if industry is not None]\n",
    "free = [industry for industry in free if industry is not None]\n",
    "pay = [industry for industry in pay if industry is not None]\n",
    "\n",
    "print(\"=== Unique industries ===\")\n",
    "print(\"전체 업종:\", all_industries)\n",
    "print(\"쇼핑 업종:\", shoping)\n",
    "print(\"교통 업종:\", traffic)\n",
    "print(\"여유 업종:\", free)\n",
    "print(\"납부 업종:\", pay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     납부     쇼핑  사교활동  교육    교통  해외  의료  여유생활  일상생활  요식\n",
      "0  1928   1408   672   0     0   0   0     0     0   0\n",
      "1  2083   2158     0   0  1659   0   0     0     0   0\n",
      "2     0  16924  1539   0  1362   0   0     0     0   0\n",
      "3  2284   2405   774   0     0   0   0     0     0   0\n",
      "4     0      0     0   0     0   0   0     0     0   0\n"
     ]
    }
   ],
   "source": [
    "industries_df = pd.DataFrame(0, index=df.index, columns=all_industries)\n",
    "\n",
    "# Process industry data\n",
    "for i in range(1, 4):\n",
    "    #industry\n",
    "    industry_col_name = f\"_{i}순위업종\"\n",
    "    amount_col_name = f\"_{i}순위업종_이용금액\"\n",
    "    mask = df[industry_col_name].notnull()\n",
    "    \n",
    "    for idx in df[mask].index:\n",
    "        industry = df.loc[idx, industry_col_name]\n",
    "        amount = df.loc[idx, amount_col_name]\n",
    "        if industry in all_industries:\n",
    "            industries_df.loc[idx, industry] = amount\n",
    "\n",
    "print(industries_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processed shoping industry data ===\n",
      "   쇼핑기타   도소매   온라인   마트  슈퍼마켓  편의점  백화점  아울렛\n",
      "0   242     0     0    0     0    0    0    0\n",
      "1     0   645     0    0   435  315    0    0\n",
      "2     0  1038  6094  924     0    0    0    0\n",
      "3     0     0     0  801   487    0    0    0\n",
      "4     0     0     0    0     0    0    0    0\n",
      "=== Processed traffic industry data ===\n",
      "    택시    주유  버스지하철  정비  철도버스\n",
      "0  200     0    105   0     0\n",
      "1    0  1215      0   0   443\n",
      "2    0  1362      0   0     0\n",
      "3  208     0      0   0     0\n",
      "4    0     0      0   0     0\n",
      "=== Processed free industry data ===\n",
      "   공연  여유기타  운동  Pet  항공  숙박  공원\n",
      "0   0     0   0    0   0   0   0\n",
      "1   0     0   0    0   0   0   0\n",
      "2   0     0   0    0   0   0   0\n",
      "3   0     0   0    0   0   0   0\n",
      "4   0     0   0    0   0   0   0\n",
      "=== Processed payment industry data ===\n",
      "    보험료   통신비  관리비  납부기타  가스/전기료\n",
      "0  1883     0    0    44       0\n",
      "1     0  2083    0     0       0\n",
      "2     0     0    0     0       0\n",
      "3  2284     0    0     0       0\n",
      "4     0     0    0     0       0\n",
      "=== Processed industry data ===\n",
      "     납부     쇼핑  사교활동  교육    교통  해외  의료  여유생활  일상생활  요식  ...  운동  Pet  항공  숙박  \\\n",
      "0  1928   1408   672   0     0   0   0     0     0   0  ...   0    0   0   0   \n",
      "1  2083   2158     0   0  1659   0   0     0     0   0  ...   0    0   0   0   \n",
      "2     0  16924  1539   0  1362   0   0     0     0   0  ...   0    0   0   0   \n",
      "3  2284   2405   774   0     0   0   0     0     0   0  ...   0    0   0   0   \n",
      "4     0      0     0   0     0   0   0     0     0   0  ...   0    0   0   0   \n",
      "\n",
      "   공원   보험료   통신비  관리비  납부기타  가스/전기료  \n",
      "0   0  1883     0    0    44       0  \n",
      "1   0     0  2083    0     0       0  \n",
      "2   0     0     0    0     0       0  \n",
      "3   0  2284     0    0     0       0  \n",
      "4   0     0     0    0     0       0  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create empty dataframes for each category with 0s\n",
    "shoping_df = pd.DataFrame(0, index=df.index, columns=shoping)\n",
    "traffic_df = pd.DataFrame(0, index=df.index, columns=traffic)\n",
    "free_df = pd.DataFrame(0, index=df.index, columns=free)\n",
    "pay_df = pd.DataFrame(0, index=df.index, columns=pay)\n",
    "\n",
    "# Process shoping industry data\n",
    "for i in range(1, 4):\n",
    "    # Shoping industry\n",
    "    industry_col_name = f\"_{i}순위쇼핑업종\"\n",
    "    amount_col_name = f\"_{i}순위쇼핑업종_이용금액\"\n",
    "    mask = df[industry_col_name].notnull()\n",
    "    \n",
    "    for idx in df[mask].index:\n",
    "        industry = df.loc[idx, industry_col_name]\n",
    "        amount = df.loc[idx, amount_col_name]\n",
    "        if industry in shoping:\n",
    "            shoping_df.loc[idx, industry] = amount\n",
    "print(\"=== Processed shoping industry data ===\")\n",
    "print(shoping_df.head())\n",
    "\n",
    "# Process traffic industry data\n",
    "for i in range(1, 4):\n",
    "    industry_col_name = f\"_{i}순위교통업종\"\n",
    "    amount_col_name = f\"_{i}순위교통업종_이용금액\"\n",
    "    mask = df[industry_col_name].notnull()\n",
    "    \n",
    "    for idx in df[mask].index:\n",
    "        industry = df.loc[idx, industry_col_name]\n",
    "        amount = df.loc[idx, amount_col_name]\n",
    "        if industry in traffic:\n",
    "            traffic_df.loc[idx, industry] = amount\n",
    "\n",
    "print(\"=== Processed traffic industry data ===\")\n",
    "print(traffic_df.head())\n",
    "\n",
    "# Process free (leisure) industry data\n",
    "for i in range(1, 4):\n",
    "    industry_col_name = f\"_{i}순위여유업종\"\n",
    "    amount_col_name = f\"_{i}순위여유업종_이용금액\"\n",
    "    mask = df[industry_col_name].notnull()\n",
    "    \n",
    "    for idx in df[mask].index:\n",
    "        industry = df.loc[idx, industry_col_name]\n",
    "        amount = df.loc[idx, amount_col_name]\n",
    "        if industry in free:\n",
    "            free_df.loc[idx, industry] = amount\n",
    "\n",
    "print(\"=== Processed free industry data ===\")\n",
    "print(free_df.head())\n",
    "\n",
    "# Process payment industry data\n",
    "for i in range(1, 4):\n",
    "    industry_col_name = f\"_{i}순위납부업종\"\n",
    "    amount_col_name = f\"_{i}순위납부업종_이용금액\"\n",
    "    mask = df[industry_col_name].notnull()\n",
    "    \n",
    "    for idx in df[mask].index:\n",
    "        industry = df.loc[idx, industry_col_name]\n",
    "        amount = df.loc[idx, amount_col_name]\n",
    "        if industry in pay:\n",
    "            pay_df.loc[idx, industry] = amount\n",
    "\n",
    "print(\"=== Processed payment industry data ===\")\n",
    "print(pay_df.head())\n",
    "\n",
    "# Combine all industry dataframes with the main dataframe\n",
    "total_rank_df = pd.concat([industries_df, shoping_df, traffic_df, free_df, pay_df], axis=1)\n",
    "\n",
    "# Drop the original industry columns that we've now processed\n",
    "industry_cols_to_drop = []\n",
    "for i in range(1, 4):\n",
    "    for category in ['업종', '쇼핑업종', '교통업종', '여유업종', '납부업종']:\n",
    "        industry_cols_to_drop.append(f\"_{i}순위{category}\")\n",
    "        industry_cols_to_drop.append(f\"_{i}순위{category}_이용금액\")\n",
    "\n",
    "#df = df.drop(industry_cols_to_drop, axis=1)\n",
    "print(\"=== Processed industry data ===\")\n",
    "print(total_rank_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df 에서 industry_cols_to_drop을 제거하고, total_rank_df를 추가\n",
    "df = df.drop(industry_cols_to_drop, axis=1)\n",
    "df = pd.concat([df, total_rank_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## '최종카드론_신청경로코드'\n",
    "1: 콜센터를 통해 신청\n",
    "2: 지점 방문 신청\n",
    "7: 인터넷 웹사이트\n",
    "8: 모바일 앱 (스마트폰)\n",
    "D: 특정 제휴 채널 또는 디지털 자동화 경로 등 (회사별로 다름)\n",
    "\n",
    "원핫인코딩 Null값은 모두 0으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 최종카드론_신청경로코드 ===\n",
      "Unique values and their counts:\n",
      "8: 289\n",
      "7: 159\n",
      "1: 74\n",
      "D: 22\n",
      "2: 9\n",
      "Total unique values: 5\n",
      "NULL values: 2447\n",
      "NULL percentage: 81.57%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '최종카드론_신청경로코드')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "카드론신청경로_1 value counts:\n",
      "카드론신청경로_1\n",
      "0    2926\n",
      "1      74\n",
      "Name: count, dtype: int64\n",
      "\n",
      "카드론신청경로_2 value counts:\n",
      "카드론신청경로_2\n",
      "0    2991\n",
      "1       9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "카드론신청경로_7 value counts:\n",
      "카드론신청경로_7\n",
      "0    2841\n",
      "1     159\n",
      "Name: count, dtype: int64\n",
      "\n",
      "카드론신청경로_8 value counts:\n",
      "카드론신청경로_8\n",
      "0    2711\n",
      "1     289\n",
      "Name: count, dtype: int64\n",
      "\n",
      "카드론신청경로_D value counts:\n",
      "카드론신청경로_D\n",
      "0    2978\n",
      "1      22\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create dummy variables for 최종카드론_신청경로코드\n",
    "cardloan_dummies = pd.get_dummies(df['최종카드론_신청경로코드'], prefix='카드론신청경로')\n",
    "\n",
    "# Drop original column and concat dummy variables\n",
    "df = pd.concat([df.drop('최종카드론_신청경로코드', axis=1), cardloan_dummies], axis=1)\n",
    "\n",
    "# Convert to binary (0,1)\n",
    "cardloan_cols = [col for col in df.columns if col.startswith('카드론신청경로_')]\n",
    "df[cardloan_cols] = df[cardloan_cols].astype(int)\n",
    "\n",
    "# Print value counts for verification\n",
    "for col in cardloan_cols:\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(df[col].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이용금액대\n",
    "???\n",
    "논의 후 처리해야할것 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 이용금액대 ===\n",
      "Unique values and their counts:\n",
      "05.10만원-: 601\n",
      "01.100만원+: 552\n",
      "02.50만원+: 543\n",
      "09.미사용: 484\n",
      "04.10만원+: 438\n",
      "03.30만원+: 382\n",
      "Total unique values: 6\n",
      "NULL values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '이용금액대')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대표결제방법코드\n",
    "전부 자동이체\n",
    "drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 대표결제방법코드 ===\n",
      "Unique values and their counts:\n",
      "자동이체: 3000\n",
      "Total unique values: 1\n",
      "NULL values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '대표결제방법코드')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대표결제방법코드 drop\n",
    "df.drop('대표결제방법코드', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대표청구지고객주소구분코드\n",
    "원핫인코딩딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 대표청구서수령지구분코드 ===\n",
      "Unique values and their counts:\n",
      "우편: 1572\n",
      "이메일: 993\n",
      "당사페이앱+이메일: 312\n",
      "K톡명세서+이메일: 70\n",
      "미수신: 53\n",
      "Total unique values: 5\n",
      "NULL values: 0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'대표청구지고객주소구분코드'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\howsr\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '대표청구지고객주소구분코드'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m check_unique_values(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m대표청구서수령지구분코드\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcheck_unique_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m대표청구지고객주소구분코드\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m, in \u001b[0;36mcheck_unique_values\u001b[1;34m(df, column_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_unique_values\u001b[39m(df, column_name):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Get unique values and their counts\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     value_counts \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Check for NULL values\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     null_count \u001b[38;5;241m=\u001b[39m df[column_name]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\howsr\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\howsr\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '대표청구지고객주소구분코드'"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '대표청구서수령지구분코드')\n",
    "check_unique_values(df, '대표청구지고객주소구분코드')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "청구지_미확인 value counts:\n",
      "청구지_미확인\n",
      "0    1563\n",
      "1    1437\n",
      "Name: count, dtype: int64\n",
      "\n",
      "청구지_주거지 value counts:\n",
      "청구지_주거지\n",
      "0    1714\n",
      "1    1286\n",
      "Name: count, dtype: int64\n",
      "\n",
      "청구지_회사 value counts:\n",
      "청구지_회사\n",
      "0    2723\n",
      "1     277\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create dummy variables for 대표청구지고객주소구분코드\n",
    "address_dummies = pd.get_dummies(df['대표청구지고객주소구분코드'], prefix='청구지')\n",
    "\n",
    "# Drop original column and concatenate dummy variables\n",
    "df = pd.concat([df.drop('대표청구지고객주소구분코드', axis=1), address_dummies], axis=1)\n",
    "\n",
    "# Convert to binary (0,1)\n",
    "address_cols = [col for col in df.columns if col.startswith('청구지_')]\n",
    "df[address_cols] = df[address_cols].astype(int)\n",
    "\n",
    "# Print value counts for verification\n",
    "for col in address_cols:\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "수령지_K톡명세서+이메일 value counts:\n",
      "수령지_K톡명세서+이메일\n",
      "0    2930\n",
      "1      70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "수령지_당사페이앱+이메일 value counts:\n",
      "수령지_당사페이앱+이메일\n",
      "0    2688\n",
      "1     312\n",
      "Name: count, dtype: int64\n",
      "\n",
      "수령지_미수신 value counts:\n",
      "수령지_미수신\n",
      "0    2947\n",
      "1      53\n",
      "Name: count, dtype: int64\n",
      "\n",
      "수령지_우편 value counts:\n",
      "수령지_우편\n",
      "1    1572\n",
      "0    1428\n",
      "Name: count, dtype: int64\n",
      "\n",
      "수령지_이메일 value counts:\n",
      "수령지_이메일\n",
      "0    2007\n",
      "1     993\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create dummy variables for 대표청구서수령지구분코드\n",
    "receipt_dummies = pd.get_dummies(df['대표청구서수령지구분코드'], prefix='수령지')\n",
    "\n",
    "# Drop the original column and concatenate the dummy variables\n",
    "df = pd.concat([df.drop('대표청구서수령지구분코드', axis=1), receipt_dummies], axis=1)\n",
    "\n",
    "# Convert to binary (0,1)\n",
    "receipt_cols = [col for col in df.columns if col.startswith('수령지_')]\n",
    "df[receipt_cols] = df[receipt_cols].astype(int)\n",
    "\n",
    "# Print value counts for verification\n",
    "for col in receipt_cols:\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 청구서수령방법\n",
    "원핫인코딩딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 청구서수령방법 ===\n",
      "Unique values and their counts:\n",
      "우편: 1572\n",
      "이메일: 993\n",
      "문자메세지: 312\n",
      "K톡: 70\n",
      "미수령: 53\n",
      "Total unique values: 5\n",
      "NULL values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '청구서수령방법')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "청구지수령_K톡 value counts:\n",
      "청구지수령_K톡\n",
      "0    2930\n",
      "1      70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "청구지수령_문자메세지 value counts:\n",
      "청구지수령_문자메세지\n",
      "0    2688\n",
      "1     312\n",
      "Name: count, dtype: int64\n",
      "\n",
      "청구지수령_미수령 value counts:\n",
      "청구지수령_미수령\n",
      "0    2947\n",
      "1      53\n",
      "Name: count, dtype: int64\n",
      "\n",
      "청구지수령_우편 value counts:\n",
      "청구지수령_우편\n",
      "1    1572\n",
      "0    1428\n",
      "Name: count, dtype: int64\n",
      "\n",
      "청구지수령_이메일 value counts:\n",
      "청구지수령_이메일\n",
      "0    2007\n",
      "1     993\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create dummy variables for 청구서수령방법\n",
    "bill_receipt_dummies = pd.get_dummies(df['청구서수령방법'], prefix='청구지수령')\n",
    "\n",
    "# Drop original column and concatenate dummy variables\n",
    "df = pd.concat([df.drop('청구서수령방법', axis=1), bill_receipt_dummies], axis=1)\n",
    "\n",
    "# Convert to binary (0,1)\n",
    "bill_receipt_cols = [col for col in df.columns if col.startswith('청구지수령_')]\n",
    "df[bill_receipt_cols] = df[bill_receipt_cols].astype(int)\n",
    "\n",
    "# Print value counts for verification\n",
    "for col in bill_receipt_cols:\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OS구분 코드\n",
    "원핫 인코딩 처리리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'OS구분코드'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\howsr\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'OS구분코드'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[155], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create dummy variables for OS구분코드\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m os_dummies \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOS구분코드\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Drop original column and concatenate dummy variables\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOS구분코드\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), os_dummies], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\howsr\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\howsr\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'OS구분코드'"
     ]
    }
   ],
   "source": [
    "# Create dummy variables for OS구분코드\n",
    "os_dummies = pd.get_dummies(df['OS구분코드'], prefix='OS')\n",
    "\n",
    "# Drop original column and concatenate dummy variables\n",
    "df = pd.concat([df.drop('OS구분코드', axis=1), os_dummies], axis=1)\n",
    "\n",
    "# Fill NA values with 0\n",
    "os_cols = [col for col in df.columns if col.startswith('OS_')]\n",
    "df[os_cols] = df[os_cols].fillna(0)\n",
    "\n",
    "# Convert to integer type\n",
    "df[os_cols] = df[os_cols].astype(int)\n",
    "\n",
    "# Print value counts for verification\n",
    "for col in os_cols:\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "## '할인건수_R3M', '할인건수_B0M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 할인건수_R3M ===\n",
      "Unique values and their counts:\n",
      "1회 이상: 2758\n",
      "10회 이상: 163\n",
      "20회 이상: 67\n",
      "30회 이상: 12\n",
      "Total unique values: 4\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 할인건수_B0M ===\n",
      "Unique values and their counts:\n",
      "1회 이상: 2983\n",
      "10회 이상: 17\n",
      "Total unique values: 2\n",
      "NULL values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '할인건수_R3M')\n",
    "check_unique_values(df, '할인건수_B0M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "##  '인입횟수_ARS_R6M', '이용메뉴건수_ARS_R6M', '방문횟수_PC_R6M', '방문일수_PC_R6M', '방문횟수_앱_R6M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: 인입횟수_ARS_R6M ===\n",
      "Unique values and their counts:\n",
      "1회 이상: 2929\n",
      "10회 이상: 71\n",
      "Total unique values: 2\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 이용메뉴건수_ARS_R6M ===\n",
      "Unique values and their counts:\n",
      "1회 이상: 2919\n",
      "10회 이상: 81\n",
      "Total unique values: 2\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 방문횟수_PC_R6M ===\n",
      "Unique values and their counts:\n",
      "1회 이상: 2833\n",
      "10회 이상: 84\n",
      "20회 이상: 68\n",
      "30회 이상: 15\n",
      "Total unique values: 4\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 방문일수_PC_R6M ===\n",
      "Unique values and their counts:\n",
      "1회 이상: 2846\n",
      "10회 이상: 111\n",
      "20회 이상: 43\n",
      "Total unique values: 3\n",
      "NULL values: 0\n",
      "\n",
      "=== Analysis for column: 방문횟수_앱_R6M ===\n",
      "Unique values and their counts:\n",
      "1회 이상: 2690\n",
      "10회 이상: 93\n",
      "30회 이상: 62\n",
      "20회 이상: 59\n",
      "40회 이상: 42\n",
      "60회 이상: 26\n",
      "50회 이상: 19\n",
      "70회 이상: 8\n",
      "80회 이상: 1\n",
      "Total unique values: 9\n",
      "NULL values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, '인입횟수_ARS_R6M')\n",
    "check_unique_values(df, '이용메뉴건수_ARS_R6M')\n",
    "check_unique_values(df, '방문횟수_PC_R6M')\n",
    "check_unique_values(df, '방문일수_PC_R6M')\n",
    "check_unique_values(df, '방문횟수_앱_R6M')\n",
    "check_unique_values(df, '캠페인접촉건수_R12M')\n",
    "check_unique_values(df, '캠페인접촉일수_R12M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis for column: OS구분코드 ===\n",
      "Unique values and their counts:\n",
      "Android: 697\n",
      "IOS: 232\n",
      "Total unique values: 2\n",
      "NULL values: 2071\n",
      "NULL percentage: 69.03%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_unique_values(df, 'OS구분코드')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
