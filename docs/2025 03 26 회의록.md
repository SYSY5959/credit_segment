---
날짜: 2025-03-26
완료: false
tags:
  - credit
---

id 하나당 6개월
데이터 프레임이 8개
각 데이터 프레임에 30~100개에 컬럼이 있는데
NULL 값있는것도 있어

## EDA 
일단은 한명에 데이터를 만들어서 컬럼들의 관계를 파악
시계열관점에서 6개월동안에 변화를 확인해야하고 (8개 각각)
1개월 관점에서 8개 데이터 프레임의 관계를 파악을 해야함

-> 모델링
뭐가지고 할건지, 피처엔지니어링, 전처리 해야하고, 이상치 잡아야해


1. EDA
2. 데이터 통합(전처리)
3. 모델 조사
	1. xgboost, lightGBM, catboost 돌여야함
	2. **TabNet** (Google Brain, 2019)과 **FT-Transformer**(Feature Tokenizer + Transformer, 2021) 등은 **정형 데이터를 위한 특화 신경망**으로 주목받았습니다.
	3. **Transformer**를 정형 데이터에 적용한 **TabTransformer**나 **SAINT** 모델 등은 **자기어텐션으로 범주형 변수의 문맥을 학습**하여 성능 향상을 노립니다
4. 피처엔지니어링 
	1. 190개 변수로부터 **수천 개의 파생 변수**(집계, 차분, 비율, 랙 특성 등)을 만들어낸 후
	2. (1) 중요도가 **0인 피처 제거**, 
	3. (2) **계층적 퍼뮤테이션 중요도** 단계적 제거,
	4. (3) **퍼뮤테이션 중요도** 재평가, 
	5. (4) **전진 선택법** 적용, 
	6. (5) 시계열 검증 등을 통해 **최종 2,500개 피처**로 축소했습니다​
5. Recursive Feature Elimination (RFE)
	1.  **재귀적 특성 제거** 역시 많이 쓰입니다. **SVM-RFE** 등은 피처를 하나씩 제거하며 모델 성능 변화를 관찰하여 최적 피처subset을 찾는데, **신용카드 사기 탐지** 연구에서 SVM-RFE로 성능을 끌어올린 예시가 있습니다​. RFE는 교차검증(RFECV)과 함께 쓰여 **과적합을 방지**하면서 중요한 특성을 골라낼 수 있습니다.
6. 차원 축소와 임베딩
	1. PCA(주성분 분석)나 t-SNE 같은 차원 축소 기법도 **고차원 금융 데이터 전처리**에 활용
	2. PCA로 축소 후 LightGBM + SMOTEENN 조합이 최고 성능
	3. **범주형 변수 임베딩**도 중요한데, One-Hot 인코딩으로 차원이 크게 늘어날 경우 **임베딩 레이어**를 둬서 저차원 연속 벡터로 변환하면 성능이 좋아진다는 사례가 많습니다.
	4. **TabTransformer/FT-Transformer**는 범주형 변수를 **임베딩 후 Transformer로 문맥화**하는 전


## 일정
### **[세부일정]**
**4/13 종료 목표**

## 3/28
- 데이터통합
- 데이터 전처리
- EDA 피처중에 쓰잘대기 없는것들 솎아내는거 위주로
- dataframe 별로 특징 조사 

## 4/2
- 1차 모델링 (베이스라인)
- EDA
	- 시계열 관점
	- ID 관점
	- 8개 상관관계